{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# standard python\n",
    "import numpy as np\n",
    "import scipy\n",
    "#import pathlib\n",
    "\n",
    "import os\n",
    "# plotting, especially for jupyter notebooks\n",
    "import matplotlib\n",
    "#matplotlib.rcParams['text.usetex'] = True # breaks for some endpoint labels\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution() # needed for tf version 1 or it stages operations but does not do them\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "# local routines\n",
    "#from chemdataprep import load_PDBs,load_countsfromPDB,load_diametersfromPDB,find_chemnames\n",
    "#from toxmathandler import load_tscores\n",
    "\n",
    "#checkpoint_path = \"/home2/ajgreen4/Read-Across_w_GAN/Models/cp.ckpt\"\n",
    "#checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "print(\"tensorflow version\",tf.__version__,\". Executing eagerly?\",tf.executing_eagerly())\n",
    "print(\"Number of GPUs: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python 3.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qm7_weightedviews as CP\n",
    "import scipy.io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qm7_data = scipy.io.loadmat('qm7.mat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the dimensions of each array\n",
    "for key in ['X', 'R', 'Z', 'T', 'P']:\n",
    "    array = qm7_data.get(key)\n",
    "    if array is not None:\n",
    "        print(f\"Dimensions of {key}: {array.shape}\")\n",
    "    else:\n",
    "        print(f\"{key} not found in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the atomic numbers array\n",
    "# Z is numpy array that might be multi-dimensional so its better to convert it to one-dimensional array\n",
    "atomic_numbers = qm7_data['Z']\n",
    "all_atomic_numbers = atomic_numbers.flatten()\n",
    "\n",
    "# Find atomic numbers\n",
    "unique_atomic_numbers = set(all_atomic_numbers)\n",
    "unique_atomic_numbers.discard(0)  # Remove 0 if it's not a valid atomic number\n",
    "\n",
    "# Print unique atomic numbers\n",
    "print(\"atomic numbers:\", unique_atomic_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def atomic_number_to_symbol(atomic_number):\n",
    "    periodic_table = {\n",
    "        1: 'H', 6: 'C', 7: 'N', 8: 'O', 16: 'S'\n",
    "    }\n",
    "    return periodic_table.get(atomic_number, 'Unknown')\n",
    "\n",
    "# Convert atomic numbers to element symbols\n",
    "unique_elements = {atomic_number_to_symbol(int(num)) for num in unique_atomic_numbers}\n",
    "\n",
    "\n",
    "Z = qm7_data['Z']\n",
    "R = qm7_data['R']\n",
    "mollist = []\n",
    "\n",
    "for mol_idx in range(Z.shape[0]):\n",
    "    molecule = []\n",
    "    for atom_idx in range(Z.shape[1]):\n",
    "        atomic_number = Z[mol_idx, atom_idx]\n",
    "        if atomic_number != 0:\n",
    "            atom_type = atomic_number_to_symbol(atomic_number)\n",
    "            # Ensure coordinates are a NumPy array\n",
    "            coordinates = np.array(R[mol_idx, atom_idx, :])\n",
    "            molecule.append((atom_type, coordinates))\n",
    "    mollist.append(molecule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mollist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def speciesmap(atom_type):\n",
    "    atom_to_number = {'H': 1, 'C': 6, 'N': 7, 'O': 8, 'S': 16}\n",
    "    #print(atom_type)\n",
    "    return np.array([atom_to_number.get(atom_type, 0)])  # Returns 0 if atom type is not recognized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1) Full split to get views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from qm7_weightedviews import load_qm7_data\n",
    "ws, vs, Natoms, Nviews = load_qm7_data(mollist, speciesmap, setNatoms=None, setNviews=None, carbonbased=False, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2) Heaviest atom as origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from qm7_weightedviews import load_qm7_data\n",
    "#ws, vs, Natoms, Nviews = load_qm7_data(mollist, speciesmap, setNatoms=None, setNviews=None, carbonbased=False, not_hydrogen= False, heaviest_origin= True, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Carbon as origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from qm7_weightedviews import load_qm7_data,structuretoviews\n",
    "#ws, vs, Natoms, Nviews = load_qm7_data(mollist, speciesmap, setNatoms=None, setNviews=None, carbonbased= True, not_hydrogen= False, heaviest_origin= False, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Hydrogen not as origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from qm7_weightedviews import load_qm7_data\n",
    "#ws, vs, Natoms, Nviews = load_qm7_data(mollist, speciesmap, setNatoms=None, setNviews=None, carbonbased=False, not_hydrogen= True, heaviest_origin= False, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T=qm7_data['T']\n",
    "T_reshaped = T.reshape((7165,))\n",
    "print(type(T_reshaped))\n",
    "T_reshaped.shape\n",
    "from sklearn.model_selection import train_test_split\n",
    "Z = qm7_data['Z']\n",
    "R = qm7_data['R']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "#usual routine, training to testing ratio is 80:20\n",
    "Z_train, Z_test, R_train, R_test, T_train, T_test = train_test_split(Z, R,T_reshaped, test_size=0.2, random_state=42)\n",
    "\n",
    "def atomic_number_to_symbol(atomic_number):\n",
    "    periodic_table = {1: 'H', 6: 'C', 7: 'N', 8: 'O', 16: 'S'}\n",
    "    return periodic_table.get(atomic_number, 'Unknown')\n",
    "def convert_to_mollist1(Z_data, R_data):\n",
    "    mollist1 = []\n",
    "    for mol_idx in range(Z_data.shape[0]):\n",
    "        molecule = []\n",
    "        for atom_idx in range(Z_data.shape[1]):\n",
    "            atomic_number = Z_data[mol_idx, atom_idx]\n",
    "            if atomic_number != 0:  # Assuming 0 means no atom present\n",
    "                atom_type = atomic_number_to_symbol(atomic_number)\n",
    "                coordinates = np.array(R_data[mol_idx, atom_idx, :])\n",
    "                molecule.append((atom_type, coordinates))\n",
    "        mollist1.append(molecule)\n",
    "    return mollist1\n",
    "\n",
    "# Convert training and testing data to mollist format\n",
    "qm7_train = convert_to_mollist1(Z_train, R_train)\n",
    "qm7_test = convert_to_mollist1(Z_test, R_test)\n",
    "qm7_labels_train = T_train\n",
    "qm7_labels_test = T_test\n",
    "\n",
    "#qm7_labels_train = [chemicaldiameter(x) for x in qm7_train]\n",
    "#qm7_labels_test = [chemicaldiameter(x) for x in qm7_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(T_reshaped))\n",
    "print(np.max(T_reshaped))\n",
    "print(np.mean(T_reshaped))\n",
    "print(np.std(T_reshaped))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(T_reshaped)\n",
    "plt.title(\"T_reshaped\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(T_reshaped, bins=30)\n",
    "plt.title(\"T\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ws_qm7_train, vs_qm7_train, Natoms_train, Nviews_train = load_qm7_data(qm7_train, speciesmap, setNatoms=None, setNviews=None, carbonbased= False, not_hydrogen= False, heaviest_origin= False, verbose=0)\n",
    "qm7G_train=[ws_qm7_train, vs_qm7_train]\n",
    "ws_qm7_test, vs_qm7_test, Natoms_test, Nviews_test = load_qm7_data(qm7_test, speciesmap, setNatoms=None, setNviews=None, carbonbased=False, not_hydrogen= False, heaviest_origin= False, verbose=0)\n",
    "qm7G_test=[ws_qm7_test, vs_qm7_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ws_qm7_train.shape)\n",
    "print(vs_qm7_train.shape)\n",
    "print(ws_qm7_test.shape)\n",
    "print(vs_qm7_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsG_train = qm7_labels_train\n",
    "labelsG_test = qm7_labels_test\n",
    "Ntoxicity = 1\n",
    "ws_train, vs_train = ws_qm7_train, vs_qm7_train\n",
    "ws_test, vs_test = ws_qm7_test, vs_qm7_test\n",
    "dataG_train=[ws_train,vs_train]\n",
    "dataG_test=[ws_test,vs_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(labelsG_train))\n",
    "print(type(labelsG_test))\n",
    "print(type(dataG_train))\n",
    "print(type(dataG_test))\n",
    "print(ws_train.shape)\n",
    "print(vs_train.shape)\n",
    "print(ws_test.shape)\n",
    "print(vs_test.shape)\n",
    "print(labelsG_train.shape)\n",
    "print(labelsG_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##################################################################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Code\n",
    "## Constructor routines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic dense NN\n",
    "def multiDense(Nin,Nout,Nhidden,widthhidden=None,kernel_regularizer=None):\n",
    "    \"\"\"Construct a basic NN with some dense layers.\n",
    "    \n",
    "    :parameter Nin: The number of inputs\n",
    "    :type Nin: int\n",
    "    :parameter Nout: The number of outputs\n",
    "    :type Nout: int\n",
    "    :parameter Nhidden: The number of hidden layers.\n",
    "    :type Nhidden: int\n",
    "    :parameter widthhidden: The width of each hidden layer.\n",
    "        If left at None, Nin + Nout will be used.\n",
    "    :parameter kernel_regularizer: the regularizer to use, such as regularizers.l2(0.001)\n",
    "    :type kernel_regularizer: tensorflow.keras.regularizers.xxx\n",
    "    :returns: The NN model\n",
    "    :rtype: keras.Model\n",
    "    \n",
    "    \"\"\"\n",
    "    if widthhidden is None:\n",
    "        widthhidden = Nin + Nout\n",
    "    x = inputs = keras.Input(shape=(Nin,), name='multiDense_input')\n",
    "    if kernel_regularizer is not None:\n",
    "        print(\"Using regularization\")\n",
    "    for i in range(Nhidden):\n",
    "        x = layers.Dense(widthhidden, activation='relu', kernel_regularizer=kernel_regularizer,name='dense'+str(i))(x)\n",
    "#        x = layers.Dense(widthhidden, name='dense'+str(i))(x)\n",
    "#        x = tf.nn.leaky_relu(x, alpha=0.05)\n",
    "#    outputs = layers.Dense(Nout, activation='linear',name='multiDense_output')(x)\n",
    "    outputs = layers.Dense(Nout,name='multiDense_output')(x)\n",
    "    #outputs = tf.nn.leaky_relu(outputs, alpha=0.05)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)#, name='multiDense')\n",
    "if 1:\n",
    "    # manual check of multiDense\n",
    "    mmd = multiDense(92,3,4,10)\n",
    "    mmd.summary()\n",
    "    # used to do the weighted sum over views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelwrapper(Nparallel,basemodel,insteadmax=False):\n",
    "    \"\"\"Construct a model that applies a basemodel multiple times and take a weighted sum (or max) of the result.\n",
    "    \n",
    "    :parameter Nparallel: The number of times to apply in parallel\n",
    "    :type Nparallel: int\n",
    "    :parameter basemodel: a keras.Model inferred to have Nin inputs and Nout outputs.\n",
    "    :type basemodel: a keras.Model\n",
    "    :parameter insteadmax: If True, take the max of the results of the basemodel instead of the weighted sum.\n",
    "        For compatibility, the model is still constructed with weights as inputs, but it ignores them.\n",
    "    :type insteadmax: Boolean\n",
    "    :returns: model with inputs shape [(?,Nparallel),(?,Nin,Nparallel)] and outputs shape (?,Nout).\n",
    "        The first input is the scalar weights in the sum.\n",
    "    :rtype: keras.Model\n",
    "    \n",
    "    Note: We could do a max over the parallel applications instead of or in addition to the weighted sum.\n",
    "    \n",
    "    \"\"\"\n",
    "    # infer shape of basemodel inputs and outputs\n",
    "    Nin =  basemodel.inputs[0].shape[1]\n",
    "    Nout =  basemodel.outputs[0].shape[1]\n",
    "    \n",
    "    # Apply basemodel Nparallel times in parallel\n",
    "    # create main input (?,Nparallel,Nin) \n",
    "    parallel_inputs = keras.Input(shape=(Nparallel,Nin), name='parallelwrapper_input0')\n",
    "    # apply base NN to each parallel slice; outputs (?,Nparallel,Nout)\n",
    "    if False:\n",
    "        # original version, stopped working at some tensorflow update\n",
    "        xb = basemodel(parallel_inputs) # worked in earlier tensorflow\n",
    "        #xb = tf.map_fn(basemodel,parallel_inputs) # another version that fails\n",
    "    else:\n",
    "        # newer version, works but makes summary and graphing cumbersome\n",
    "        # unstack in the Nparallel directio\n",
    "        parallel_inputsunstacked = tf.keras.ops.unstack(parallel_inputs, Nparallel, 1)\n",
    "        # apply base NN to each \n",
    "        xbunstacked = [basemodel(x) for x in parallel_inputsunstacked]\n",
    "        # re-stack\n",
    "        xb = tf.keras.ops.stack(xbunstacked,axis=1)\n",
    "    \n",
    "    # create input scalars for weighted sun (?,Nparallel)\n",
    "    weight_inputs = keras.Input(shape=(Nparallel,), name='parallelScalars')\n",
    "    if insteadmax:\n",
    "        # take max over the Nparallel direction to get (?,1,Nout)\n",
    "        out = layers.MaxPool1D(pool_size=Nparallel)(xb)\n",
    "        # reshape to (?,Nout)\n",
    "        out = layers.Reshape((Nout,))(out)\n",
    "    else:\n",
    "        # do a weighted sum over the Nparallel direction to get (?,Nout)\n",
    "        out = layers.Dot((-2,-1))([xb,weight_inputs])\n",
    "    \n",
    "    return keras.Model(inputs=[weight_inputs,parallel_inputs], outputs=out, name='parallelwrapper')\n",
    "if 1:\n",
    "    # manual check\n",
    "    mmd = multiDense(92,3,3,10)\n",
    "    mpw = parallelwrapper(23,mmd,insteadmax=0)\n",
    "    mpw.summary()\n",
    "    # make models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_generator(data,labels,baselayers,Nfeatures,endlayers,base_regularizer=None,end_regularizer=None):\n",
    "    \"\"\"Initialize the generator neural net.\n",
    "    \n",
    "    :returns: return generator and descrimina NN.\n",
    "    :rtype: keras.Model\n",
    "    \n",
    "    \"\"\"\n",
    "    ## Option changing how results of each view are aggregated\n",
    "    insteadmax = False # Does weighted average; original design\n",
    "    #insteadmax = True # Does max instead of weighted average (for both G and D)\n",
    "\n",
    "    # G\n",
    "    # base NN\n",
    "    Gbase = multiDense(data[1].shape[2],Nfeatures,baselayers,kernel_regularizer=base_regularizer) \n",
    "    # parallel view wrapper\n",
    "    Gpw = parallelwrapper(Nviews,Gbase,insteadmax)\n",
    "    # features to toxicity\n",
    "    #Gft = multiDense(Nfeatures,labels.shape[1],endlayers,kernel_regularizer=end_regularizer)\n",
    "    #we got an error when running original line since energies are one dimnsional array so we can interpret the single dimension as having only one dimension\n",
    "    Gft = multiDense(Nfeatures,1,endlayers,kernel_regularizer=end_regularizer) \n",
    "    # string together\n",
    "    generator = keras.Model(inputs=Gpw.inputs,outputs=Gft(Gpw.outputs),name='generator')\n",
    "    # make trainable\n",
    "    generator.compile(optimizer='adam',loss='mse')\n",
    "    #generator.summary()\n",
    "    # previously did better with Nfeatures=Ntoxicity and no Gft\n",
    "\n",
    "    if 0:\n",
    "        # sanity checks that model is working\n",
    "        print(\"Sanity check:\")\n",
    "        ws,vs = data\n",
    "        gbv0call = Gbase(vs[:,0,:]).numpy()\n",
    "        gbv0predict = Gbase.predict(vs[:,0,:])\n",
    "        print(\"base: 0 ?==\", np.linalg.norm(gbv0call-gbv0predict))\n",
    "        gpwcall = Gpw([ws,vs]).numpy()\n",
    "        gpwpredict = G\n",
    "        pw.predict([ws,vs])\n",
    "        print(\"wrapper: 0 ?==\",np.linalg.norm(gpwcall-gpwpredict))\n",
    "        gencall = generator([ws,vs]).numpy()\n",
    "        genpredict = generator.predict([ws,vs])\n",
    "        print(\"whole: 0 ?==\",np.linalg.norm(gencall-genpredict))\n",
    "        \n",
    "    return generator\n",
    "baselayers = 2  # hidden layers before weighted sum\n",
    "base_reg = 0.1  # regularization for the base layers\n",
    "Nfeatures = 3  # number of outputs of weighted sum\n",
    "endlayers = 2   # hidden layers after weighted sum\n",
    "end_reg = 0.1  # regularization for the end layers\n",
    "\n",
    "if base_reg == 0:\n",
    "    base_regularizer = None\n",
    "else:\n",
    "    base_regularizer = regularizers.l2(base_reg)\n",
    "\n",
    "if end_reg == 0:\n",
    "    end_regularizer = None\n",
    "else:\n",
    "    end_regularizer = regularizers.l2(end_reg)  #???\n",
    "\n",
    "print(\"(baselayers, base_reg, Nfeatures, endlayers, end_reg) =\",\n",
    "      (baselayers, base_reg, Nfeatures, endlayers, end_reg))\n",
    "# compile model with options\n",
    "generator = init_generator(dataG_train,labelsG_train,baselayers,Nfeatures,endlayers,\n",
    "                           base_regularizer=base_regularizer,end_regularizer=end_regularizer)\n",
    "generator.compile(optimizer='adam',loss='mse')\n",
    "#generator.summary()\n",
    "#change loss to MAE for energies\n",
    "dataG_train = [np.array(dataG_train[0], dtype='float32'), np.array(dataG_train[1], dtype='float32')]\n",
    "\n",
    "# Ensure labels are numpy arrays of type float32 and have a 2D shape if required\n",
    "labelsG_train = np.array(labelsG_train, dtype='float32').reshape(-1, 1)\n",
    "dataG_test = [np.array(dataG_test[0], dtype='float32'), np.array(dataG_test[1], dtype='float32')]\n",
    "labelsG_test = np.array(labelsG_test, dtype='float32').reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# fit\n",
    "BATCH_SIZE = 32\n",
    "if 1: \n",
    "    history = generator.fit(dataG_train,labelsG_train,\n",
    "                  epochs=100,batch_size=BATCH_SIZE,verbose=1,\n",
    "                  validation_data=(dataG_test, labelsG_test))\n",
    "    print(\"train loss=\",generator.evaluate(dataG_train,labelsG_train,verbose=1))\n",
    "    print(\"test loss=\",generator.evaluate(dataG_test,labelsG_test,verbose=1))\n",
    "#     generator.fit(dataG_train,labelsG_train,epochs=1000,batch_size=BATCH_SIZE,verbose=0)\n",
    "#    generator.save(modelpath+\"AG-model-RT.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract loss values\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(train_loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_train = generator.predict(dataG_train)\n",
    "predicted_test = generator.predict(dataG_test)\n",
    "\n",
    "# Calculate the Mean Absolute Error for the training set\n",
    "mae_train = np.mean(np.abs(predicted_train - labelsG_train))\n",
    "\n",
    "# Calculate the Mean Absolute Error for the test set\n",
    "mae_test = np.mean(np.abs(predicted_test - labelsG_test))\n",
    "\n",
    "# Print out the MAE for both sets\n",
    "print(\"Train Mean Absolute Error: \", mae_train)\n",
    "print(\"Test Mean Absolute Error: \", mae_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from tensorflow.keras import regularizers\n",
    "from docx import Document\n",
    "from IPython.display import FileLink\n",
    "\n",
    "\n",
    "baselayers_options = [1,3,5]       # hidden layers before weighted sum\n",
    "base_reg_options    = [0,0.2,0.3] # regularization for the base layers\n",
    "endlayers_options   = [1,3,5]       # hidden layers after weighted sum\n",
    "end_reg_options     = [0, 0.2]   # regularization for the end layers\n",
    "Nfeatures = 1 \n",
    "\n",
    "best_mae = float('inf')\n",
    "best_params = None\n",
    "results = []\n",
    "\n",
    "for baselayers, base_reg, endlayers, end_reg in itertools.product(\n",
    "        baselayers_options, base_reg_options, endlayers_options, end_reg_options):\n",
    "    \n",
    "    \n",
    "    base_regularizer = None if base_reg == 0 else regularizers.l2(base_reg)\n",
    "    end_regularizer = None if end_reg == 0 else regularizers.l2(end_reg)\n",
    "    \n",
    "    \n",
    "    generator = init_generator(dataG_train, labelsG_train, baselayers, Nfeatures, endlayers,\n",
    "                               base_regularizer=base_regularizer, end_regularizer=end_regularizer)\n",
    "    generator.compile(optimizer='adam', loss='mse')  # compile with MSE loss\n",
    "    \n",
    "  \n",
    "    history = generator.fit(dataG_train, labelsG_train,\n",
    "                            epochs=200,  # adjust epochs as needed\n",
    "                            batch_size=32,\n",
    "                            verbose=0,\n",
    "                            validation_data=(dataG_test, labelsG_test))\n",
    "    \n",
    "   \n",
    "    test_loss = generator.evaluate(dataG_test, labelsG_test, verbose=0)\n",
    "    \n",
    "    \n",
    "    predicted_train = generator.predict(dataG_train)\n",
    "    predicted_test  = generator.predict(dataG_test)\n",
    "    \n",
    "    \n",
    "    mae_train = np.mean(np.abs(predicted_train - labelsG_train))\n",
    "    mae_test  = np.mean(np.abs(predicted_test - labelsG_test))\n",
    "    \n",
    "    results.append({\n",
    "        'baselayers': baselayers,\n",
    "        'base_reg': base_reg,\n",
    "        'endlayers': endlayers,\n",
    "        'end_reg': end_reg,\n",
    "        'test_loss': test_loss,\n",
    "        'mae_train': mae_train,\n",
    "        'mae_test': mae_test\n",
    "    })\n",
    "    \n",
    "    \n",
    "    if mae_test < best_mae:\n",
    "        best_mae = mae_test\n",
    "        best_params = (baselayers, base_reg, endlayers, end_reg, test_loss, mae_train, mae_test)\n",
    "\n",
    "print(\"Best parameters:\")\n",
    "print(\"  baselayers =\", best_params[0])\n",
    "print(\"  base_reg   =\", best_params[1])\n",
    "print(\"  endlayers  =\", best_params[2])\n",
    "print(\"  end_reg    =\", best_params[3])\n",
    "print(\"Test Loss (MSE): {:.4f}\".format(best_params[4]))\n",
    "print(\"Train MAE:      {:.4f}\".format(best_params[5]))\n",
    "print(\"Test MAE:       {:.4f}\".format(best_params[6]))\n",
    "\n",
    "\n",
    "print(\"\\nAll grid search results:\")\n",
    "for res in results:\n",
    "    print(res)\n",
    "\n",
    "document = Document()\n",
    "document.add_heading('Grid Search Results', level=1)\n",
    "\n",
    "\n",
    "table = document.add_table(rows=1, cols=5)\n",
    "hdr_cells = table.rows[0].cells\n",
    "hdr_cells[0].text = 'baselayers'\n",
    "hdr_cells[1].text = 'base_reg'\n",
    "hdr_cells[2].text = 'endlayers'\n",
    "hdr_cells[3].text = 'end_reg'\n",
    "hdr_cells[4].text = 'Test MAE'\n",
    "\n",
    "# Add each result as a new row in the table\n",
    "for res in results:\n",
    "    row_cells = table.add_row().cells\n",
    "    row_cells[0].text = str(res['baselayers'])\n",
    "    row_cells[1].text = str(res['base_reg'])\n",
    "    row_cells[2].text = str(res['endlayers'])\n",
    "    row_cells[3].text = str(res['end_reg'])\n",
    "    row_cells[4].text = f\"{res['mae_test']:.4f}\"\n",
    "\n",
    "\n",
    "doc_filename = 'grid_search_results.docx'\n",
    "document.save(doc_filename)\n",
    "print(f\"Results saved to {doc_filename}\")\n",
    "\n",
    "\n",
    "display(FileLink(doc_filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot predictions of G versus truth\n",
    "def PvT_plot(model,data,labels,legend=None,title=None,doresidual=False):\n",
    "    \"\"\"Construct a plot of the true labels (x-axis) vs the data generated by the model (y-axis).\n",
    "    \n",
    "    :parameter model: the model (e.g. NN)\n",
    "    :type model: keras.model\n",
    "    :parameter data: the data that can be input to the model\n",
    "    :type data: numpy.array\n",
    "    :parameter labels: the true outputs corresponding to the data\n",
    "    :type labels: numpy.array\n",
    "    :parameter legend: The string labels corresponding to the columns of labels \n",
    "    :type legend: None or list of str\n",
    "    :parameter title: A title for the plot\n",
    "    :type title: None or string\n",
    "    :parameter doresidual: If true, plot the residual instead\n",
    "    :type doresidual: Boolean\n",
    "    \"\"\"\n",
    "        \n",
    "    gen_lab = model.predict(data)\n",
    "    if doresidual:\n",
    "        gen_lab = gen_lab - labels\n",
    "\n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)        \n",
    "    if legend is None:\n",
    "        for i in range(labels.shape[1]):\n",
    "            plt.plot(labels[:,i],gen_lab[:,i],'o')\n",
    "    else:\n",
    "        for i in range(labels.shape[1]):\n",
    "            # include legend\n",
    "            plt.plot(labels[:,i],gen_lab[:,i],'o', label=legend[i])\n",
    "        ax.legend(bbox_to_anchor=(1.04,1), borderaxespad=0)\n",
    "    ax.set_xlabel('True Values')\n",
    "    if doresidual:\n",
    "        ax.set_ylabel('Residual Values')\n",
    "    else:\n",
    "        ax.set_ylabel('Generated Values')\n",
    "        # reference line\n",
    "        mintrue = np.min(labels)\n",
    "        maxtrue = np.max(labels)\n",
    "        plt.plot([mintrue,maxtrue],[mintrue,maxtrue])\n",
    "    if title is None:\n",
    "        title = ''\n",
    "    if 1:\n",
    "        if not doresidual:\n",
    "            gen_lab = gen_lab - labels\n",
    "        MSE = np.square(gen_lab).mean()\n",
    "        title = title+\" Mean Squared Error=\"+str(MSE)\n",
    "        print(\"Mean Squared Error: \", MSE)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doresidual = False\n",
    "legend=None\n",
    "PvT_plot(generator,dataG_train,labelsG_train,title='train',doresidual=doresidual,legend=legend)\n",
    "PvT_plot(generator,dataG_test,labelsG_test,title='test',doresidual=doresidual,legend=legend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
