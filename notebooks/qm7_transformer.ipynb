{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb20e585-8116-43c2-afff-4b7f0774ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "# standard python\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import stats\n",
    "#import pathlib\n",
    "\n",
    "import os\n",
    "# plotting, especially for jupyter notebooks\n",
    "import matplotlib\n",
    "#matplotlib.rcParams['text.usetex'] = True # breaks for some endpoint labels\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution() # needed for tf version 1 or it stages operations but does not do them\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "# local routines\n",
    "#from chemdataprep import load_PDBs,load_countsfromPDB,load_diametersfromPDB,find_chemnames\n",
    "#from toxmathandler import load_tscores\n",
    "\n",
    "#checkpoint_path = \"/home2/ajgreen4/Read-Across_w_GAN/Models/cp.ckpt\"\n",
    "#checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "print(\"tensorflow version\",tf.__version__,\". Executing eagerly?\",tf.executing_eagerly())\n",
    "print(\"Number of GPUs: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e06ae1-783c-453a-905a-7305d37bd5fe",
   "metadata": {},
   "source": [
    "# Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4be452-170b-4f8f-a6f3-0bec17926ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qm7_weightedviews as CP\n",
    "import scipy.io\n",
    "qm7_data = scipy.io.loadmat('qm7.mat')\n",
    "# Print the dimensions of each array\n",
    "for key in ['X', 'R', 'Z', 'T', 'P']:\n",
    "    array = qm7_data.get(key)\n",
    "    if array is not None:\n",
    "        print(f\"Dimensions of {key}: {array.shape}\")\n",
    "    else:\n",
    "        print(f\"{key} not found in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c455f89-5dff-4553-bdcd-bb84d0e6e8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the atomic numbers array\n",
    "# Z is numpy array that might be multi-dimensional so its better to convert it to one-dimensional array\n",
    "atomic_numbers = qm7_data['Z']\n",
    "all_atomic_numbers = atomic_numbers.flatten()\n",
    "\n",
    "# Find atomic numbers\n",
    "unique_atomic_numbers = set(all_atomic_numbers)\n",
    "unique_atomic_numbers.discard(0)  # Remove 0 if it's not a valid atomic number\n",
    "\n",
    "# Print unique atomic numbers\n",
    "print(\"atomic numbers:\", unique_atomic_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a990def4-3c20-4f39-9ccf-923da8405623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def atomic_number_to_symbol(atomic_number):\n",
    "    periodic_table = {\n",
    "        1: 'H', 6: 'C', 7: 'N', 8: 'O', 16: 'S'\n",
    "    }\n",
    "    return periodic_table.get(atomic_number, 'Unknown')\n",
    "\n",
    "# Convert atomic numbers to element symbols\n",
    "unique_elements = {atomic_number_to_symbol(int(num)) for num in unique_atomic_numbers}\n",
    "\n",
    "\n",
    "Z = qm7_data['Z']\n",
    "R = qm7_data['R']\n",
    "mollist = []\n",
    "\n",
    "for mol_idx in range(Z.shape[0]):\n",
    "    molecule = []\n",
    "    for atom_idx in range(Z.shape[1]):\n",
    "        atomic_number = Z[mol_idx, atom_idx]\n",
    "        if atomic_number != 0:\n",
    "            atom_type = atomic_number_to_symbol(atomic_number)\n",
    "            # Ensure coordinates are a NumPy array\n",
    "            coordinates = np.array(R[mol_idx, atom_idx, :])\n",
    "            molecule.append((atom_type, coordinates))\n",
    "    mollist.append(molecule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc292aad-4365-4e80-84fc-73e54821c659",
   "metadata": {},
   "outputs": [],
   "source": [
    "def speciesmap(atom_type):\n",
    "    atom_to_number = {'H': 1, 'C': 6, 'N': 7, 'O': 8, 'S': 16}\n",
    "    #print(atom_type)\n",
    "    return np.array([atom_to_number.get(atom_type, 0)])  # Returns 0 if atom type is not recognized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7324bf5f-297e-461e-8c1f-a19806a83523",
   "metadata": {},
   "source": [
    "## Full split to get views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516d95c1-ee60-4e8a-941c-84e66ea3b031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from qm7_weightedviews import load_qm7_data\n",
    "ws, vs, Natoms, Nviews = load_qm7_data(mollist, speciesmap, setNatoms=None, setNviews=None, carbonbased=False, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050ff518-a0d2-4e42-ae7f-9271f542f069",
   "metadata": {},
   "outputs": [],
   "source": [
    "T=qm7_data['T']\n",
    "T_reshaped = T.reshape((7165,))\n",
    "print(type(T_reshaped))\n",
    "T_reshaped.shape\n",
    "from sklearn.model_selection import train_test_split\n",
    "Z = qm7_data['Z']\n",
    "R = qm7_data['R']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "#usual routine, training to testing ratio is 80:20\n",
    "Z_train, Z_test, R_train, R_test, T_train, T_test = train_test_split(Z, R,T_reshaped, test_size=0.2, random_state=42)\n",
    "\n",
    "def atomic_number_to_symbol(atomic_number):\n",
    "    periodic_table = {1: 'H', 6: 'C', 7: 'N', 8: 'O', 16: 'S'}\n",
    "    return periodic_table.get(atomic_number, 'Unknown')\n",
    "def convert_to_mollist1(Z_data, R_data):\n",
    "    mollist1 = []\n",
    "    for mol_idx in range(Z_data.shape[0]):\n",
    "        molecule = []\n",
    "        for atom_idx in range(Z_data.shape[1]):\n",
    "            atomic_number = Z_data[mol_idx, atom_idx]\n",
    "            if atomic_number != 0:  # Assuming 0 means no atom present\n",
    "                atom_type = atomic_number_to_symbol(atomic_number)\n",
    "                coordinates = np.array(R_data[mol_idx, atom_idx, :])\n",
    "                molecule.append((atom_type, coordinates))\n",
    "        mollist1.append(molecule)\n",
    "    return mollist1\n",
    "\n",
    "# Convert training and testing data to mollist format\n",
    "qm7_train = convert_to_mollist1(Z_train, R_train)\n",
    "qm7_test = convert_to_mollist1(Z_test, R_test)\n",
    "qm7_labels_train = T_train\n",
    "qm7_labels_test = T_test\n",
    "\n",
    "#qm7_labels_train = [chemicaldiameter(x) for x in qm7_train]\n",
    "#qm7_labels_test = [chemicaldiameter(x) for x in qm7_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9e2bc7-d5a4-4fa5-b908-d38b0790adb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.min(T_reshaped))\n",
    "print(np.max(T_reshaped))\n",
    "print(np.mean(T_reshaped))\n",
    "print(np.std(T_reshaped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21d2666-2acf-4f6d-82b0-9713302739b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(T_reshaped)\n",
    "plt.title(\"T_reshaped\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a85f433-4138-476a-a243-6b124de05861",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(T_reshaped, bins=30)\n",
    "plt.title(\"T\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345c78e4-6a2b-491e-88c4-eba14ed1e0a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ws_qm7_train, vs_qm7_train, Natoms_train, Nviews_train = load_qm7_data(qm7_train, speciesmap, setNatoms=None, setNviews=None, carbonbased=False, not_hydrogen= False, heaviest_origin= False, verbose=0)\n",
    "qm7G_train=[ws_qm7_train, vs_qm7_train]\n",
    "ws_qm7_test, vs_qm7_test, Natoms_test, Nviews_test = load_qm7_data(qm7_test, speciesmap, setNatoms=None, setNviews=None, carbonbased=False, not_hydrogen= False, heaviest_origin= False, verbose=0)\n",
    "qm7G_test=[ws_qm7_test, vs_qm7_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cdb1cc-716f-4df5-a8dc-b593fda6ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ws_qm7_train.shape)\n",
    "print(vs_qm7_train.shape)\n",
    "print(ws_qm7_test.shape)\n",
    "print(vs_qm7_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a5c26f-26c4-48e7-9e1d-f3cc61162961",
   "metadata": {},
   "source": [
    "## Find broken views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192ecd0d-739f-4f29-9319-5947a9e4cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qm7_transformercode import small_views\n",
    "single_atom_qm7_train = small_views(vs_qm7_train)\n",
    "single_atom_qm7_test = small_views(vs_qm7_test)\n",
    "print(single_atom_qm7_train.shape)\n",
    "print(single_atom_qm7_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64039c56-3aeb-4656-845a-a309fdeb80f1",
   "metadata": {},
   "source": [
    "# Add single atom properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fed431-5656-45e5-b015-94a95839dc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qm7_transformercode import atom_properties\n",
    "from qm7_transformercode import single_atomic_property_switches\n",
    "from qm7_transformer import get_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc71ceb-3e72-4b51-88d0-24b3bbe90f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "atom_embeddings_qm7_train = get_embeddings(single_atom_qm7_train, atom_properties, single_atomic_property_switches)\n",
    "atom_embeddings_qm7_test  = get_embeddings(single_atom_qm7_test,  atom_properties, single_atomic_property_switches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dfe5d2-c673-4540-826a-7cda01289bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(atom_embeddings_qm7_train.shape)\n",
    "print(atom_embeddings_qm7_test.shape)\n",
    "print(atom_embeddings_qm7_train.dtype)\n",
    "print(atom_embeddings_qm7_test.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b51f59-deaa-4e61-827d-4ba685086046",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#atom_embeddings_qm7_train[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5c616f-630e-458b-aaaa-23595abc5464",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelsG_train = qm7_labels_train\n",
    "labelsG_test = qm7_labels_test\n",
    "Ntoxicity = 3\n",
    "ws_train, vs_train = ws_qm7_train, atom_embeddings_qm7_train\n",
    "ws_test, vs_test = ws_qm7_test, atom_embeddings_qm7_test\n",
    "dataG_train=[ws_train,vs_train]\n",
    "dataG_test=[ws_test,vs_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188beee3-e894-4708-b4a2-7f52b916dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(labelsG_train))\n",
    "print(type(labelsG_test))\n",
    "print(ws_train.shape)\n",
    "print(vs_train.shape)\n",
    "print(ws_test.shape)\n",
    "print(vs_test.shape)\n",
    "print(labelsG_train.shape)\n",
    "print(labelsG_test.shape)\n",
    "print(len(dataG_train))\n",
    "print(len(dataG_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1926091b-86fe-4806-87b7-d1c5bc39b516",
   "metadata": {},
   "source": [
    "# Transformer Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bf01ac-5011-4379-ae9a-148772bdbc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer, LayerNormalization, Dense\n",
    "\n",
    "class AtomAttention(Layer):\n",
    "    \n",
    "    def __init__(self, d_k = 32, d_v = 16 , **kwargs):\n",
    "        \n",
    "        '''\n",
    "        single head attention for atom-atom relationship\n",
    "        d_k = dimention of query and key weight matrices\n",
    "        d_v = dimension of key weight matrices (could be same as d_k)\n",
    "        \n",
    "        '''\n",
    "        super(AtomAttention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.norm = LayerNormalization(axis = -1, epsilon= 1e-6)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \n",
    "        # our current input for training is (5732,23,23,16) so (batch, views, atoms, features)\n",
    "        \n",
    "        self.batch_size = input_shape[0]\n",
    "        self.num_views = input_shape[1]\n",
    "        self.num_atoms = input_shape[2]\n",
    "        self.feature_dim  = input_shape[3]\n",
    "        \n",
    "        \n",
    "        self.W_q = self.add_weight( name = 'W_q', shape = (self.feature_dim, self.d_k), \n",
    "                                   initializer= 'glorot_uniform', trainable= True) # 16*32\n",
    "        \n",
    "        self.W_k = self.add_weight(name = 'W_k', shape=(self.feature_dim, self.d_k),\n",
    "                                   initializer = 'glorot_uniform', trainable = True) #16*32\n",
    "        \n",
    "        self.W_v = self.add_weight(name = 'W_v' , shape= (self.feature_dim, self.d_v),\n",
    "                                   initializer = 'glorot_uniform' , trainable = True) #16*16 (ideally 16*32)\n",
    "        \n",
    "        self.W_o = self.add_weight(name = 'W_o', shape = (self.d_v, self.feature_dim), \n",
    "                                   initializer= 'glorot_uniform', trainable= True) # 16*16\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        \n",
    "        '''\n",
    "        input shape = (5732,23,23,16)\n",
    "        output shape = (5732,23,23,16)\n",
    "        reshaped_input = convert batches*views into sequences to find atom-atom attention\n",
    "        '''\n",
    "        \n",
    "        reshaped_input = tf.reshape(inputs, [-1, self.num_atoms, self.feature_dim])\n",
    "        \n",
    "        Q = tf.matmul(reshaped_input, self.W_q) #sequence, atoms, d_k\n",
    "        K = tf.matmul(reshaped_input, self.W_k) # sequence, atoms, d_k\n",
    "        V = tf.matmul(reshaped_input, self.W_v) # sequence, atoms, d_v\n",
    "        \n",
    "        # find attention score\n",
    "        \n",
    "        scores = tf.matmul(Q, K, transpose_b= True)\n",
    "        \n",
    "        # scaling\n",
    "        \n",
    "        scaled_scores = scores/tf.math.sqrt(tf.cast(self.d_k, tf.float32))\n",
    "        \n",
    "        # mask rows and columns which exactly zero\n",
    "        \n",
    "        row_sums = tf.reduce_sum(scaled_scores, axis = 2)\n",
    "        col_sums = tf.reduce_sum(scaled_scores, axis = 1)\n",
    "        mask_rows = tf.equal(row_sums, 0.0)\n",
    "        mask_cols = tf.equal(col_sums, 0.0)\n",
    "        \n",
    "        mask = tf.logical_or( tf.expand_dims(mask_rows, 2), tf.expand_dims(mask_cols,1))\n",
    "        neg_inf = tf.constant(-1e9, dtype=scaled_scores.dtype)\n",
    "        scaled_scores = tf.where(mask, neg_inf, scaled_scores)\n",
    "   \n",
    "        # softmax\n",
    "        \n",
    "        attn_weight = tf.nn.softmax(scaled_scores, axis= -1)\n",
    "        \n",
    "        # multiply with V\n",
    "        \n",
    "        attn_output = tf.matmul(attn_weight, V) # sequence, atoms, d_v\n",
    "        \n",
    "        # project back to feature space\n",
    "        \n",
    "        project_output = tf.matmul(attn_output, self.W_o) # sequence, atoms, features\n",
    "        \n",
    "        # skip connection\n",
    "        \n",
    "        skip_output = project_output + reshaped_input\n",
    "        \n",
    "        # layer normalization\n",
    "        \n",
    "        normed = self.norm(skip_output)\n",
    "        \n",
    "        # get back original shape\n",
    "        \n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        \n",
    "        final_output = tf.reshape(normed, [batch_size, self.num_views, self.num_atoms, self.feature_dim])\n",
    "        \n",
    "        return final_output\n",
    "    \n",
    "# Feedforward layer\n",
    "    \n",
    "\n",
    "class FeedForward(Layer):\n",
    "    \"\"\"Position-wise feed-forward net with residual + LayerNorm.\"\"\"\n",
    "    def __init__(self, d_model=16, d_ff=64, **kwargs):\n",
    "        super(FeedForward, self).__init__(**kwargs)\n",
    "        self.fc1 = Dense(d_ff, activation='relu', name='ffn_fc1')\n",
    "        self.fc2 = Dense(d_model, name='ffn_fc2')\n",
    "        self.norm = LayerNormalization(axis=-1, epsilon=1e-6, name='ffn_norm')\n",
    "\n",
    "    def call(self, x):\n",
    "        y = self.fc1(x)\n",
    "        y = self.fc2(y)\n",
    "        return self.norm(x + y)\n",
    "\n",
    "class TransformerBlock(Layer):\n",
    "    \"\"\"Single Transformer encoder block: AtomAttention + FeedForward.\"\"\"\n",
    "    def __init__(self, d_model=16, d_k=32, d_v=16, d_ff=64, **kwargs):\n",
    "        super(TransformerBlock, self).__init__(**kwargs)\n",
    "        self.attn = AtomAttention(d_k=d_k, d_v=d_v, name='atom_attention')\n",
    "        self.ffn = FeedForward(d_model=d_model, d_ff=d_ff, name='ffn')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.attn(x)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47bdd05-25b5-43d2-8d7c-024beac2278c",
   "metadata": {},
   "source": [
    "## Neural network code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fc370d-40be-4127-a5a3-23c27db70da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic dense NN\n",
    "def multiDense(Nin,Nout,Nhidden,widthhidden=None,kernel_regularizer=None):\n",
    "    \"\"\"Construct a basic NN with some dense layers.\n",
    "    \n",
    "    :parameter Nin: The number of inputs\n",
    "    :type Nin: int\n",
    "    :parameter Nout: The number of outputs\n",
    "    :type Nout: int\n",
    "    :parameter Nhidden: The number of hidden layers.\n",
    "    :type Nhidden: int\n",
    "    :parameter widthhidden: The width of each hidden layer.\n",
    "        If left at None, Nin + Nout will be used.\n",
    "    :parameter kernel_regularizer: the regularizer to use, such as regularizers.l2(0.001)\n",
    "    :type kernel_regularizer: tensorflow.keras.regularizers.xxx\n",
    "    :returns: The NN model\n",
    "    :rtype: keras.Model\n",
    "    \n",
    "    \"\"\"\n",
    "    if widthhidden is None:\n",
    "        widthhidden = Nin + Nout\n",
    "    x = inputs = keras.Input(shape=(Nin,), name='multiDense_input')\n",
    "    if kernel_regularizer is not None:\n",
    "        print(\"Using regularization\")\n",
    "    for i in range(Nhidden):\n",
    "        x = layers.Dense(widthhidden, activation='relu', kernel_regularizer=kernel_regularizer,name='dense'+str(i))(x)\n",
    "#        x = layers.Dense(widthhidden, name='dense'+str(i))(x)\n",
    "#        x = tf.nn.leaky_relu(x, alpha=0.05)\n",
    "#    outputs = layers.Dense(Nout, activation='linear',name='multiDense_output')(x)\n",
    "    outputs = layers.Dense(Nout,name='multiDense_output')(x)\n",
    "    #outputs = tf.nn.leaky_relu(outputs, alpha=0.05)\n",
    "    return keras.Model(inputs=inputs, outputs=outputs)#, name='multiDense')\n",
    "if 1:\n",
    "    # manual check of multiDense\n",
    "    mmd = multiDense(368,3,5,15)\n",
    "    mmd.summary()\n",
    "    # used to do the weighted sum over views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bba7c7-9a77-406f-9cb7-48afff083924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelwrapper(Nparallel,basemodel,insteadmax=False):\n",
    "    \"\"\"Construct a model that applies a basemodel multiple times and take a weighted sum (or max) of the result.\n",
    "    \n",
    "    :parameter Nparallel: The number of times to apply in parallel\n",
    "    :type Nparallel: int\n",
    "    :parameter basemodel: a keras.Model inferred to have Nin inputs and Nout outputs.\n",
    "    :type basemodel: a keras.Model\n",
    "    :parameter insteadmax: If True, take the max of the results of the basemodel instead of the weighted sum.\n",
    "        For compatibility, the model is still constructed with weights as inputs, but it ignores them.\n",
    "    :type insteadmax: Boolean\n",
    "    :returns: model with inputs shape [(?,Nparallel),(?,Nin,Nparallel)] and outputs shape (?,Nout).\n",
    "        The first input is the scalar weights in the sum.\n",
    "    :rtype: keras.Model\n",
    "    \n",
    "    Note: We could do a max over the parallel applications instead of or in addition to the weighted sum.\n",
    "    \n",
    "    \"\"\"\n",
    "    # infer shape of basemodel inputs and outputs\n",
    "    Nin =  basemodel.inputs[0].shape[1]\n",
    "    Nout =  basemodel.outputs[0].shape[1]\n",
    "    \n",
    "    # Apply basemodel Nparallel times in parallel\n",
    "    # create main input (?,Nparallel,Nin) \n",
    "    parallel_inputs = keras.Input(shape=(Nparallel,Nin), name='parallelwrapper_input0')\n",
    "    # apply base NN to each parallel slice; outputs (?,Nparallel,Nout)\n",
    "    if False:\n",
    "        # original version, stopped working at some tensorflow update\n",
    "        xb = basemodel(parallel_inputs) # worked in earlier tensorflow\n",
    "        #xb = tf.map_fn(basemodel,parallel_inputs) # another version that fails\n",
    "    else:\n",
    "        # newer version, works but makes summary and graphing cumbersome\n",
    "        # unstack in the Nparallel directio\n",
    "        parallel_inputsunstacked = tf.keras.ops.unstack(parallel_inputs, Nparallel, 1)\n",
    "        # apply base NN to each \n",
    "        xbunstacked = [basemodel(x) for x in parallel_inputsunstacked]\n",
    "        # re-stack\n",
    "        xb = tf.keras.ops.stack(xbunstacked,axis=1)\n",
    "    \n",
    "    # create input scalars for weighted sun (?,Nparallel)\n",
    "    weight_inputs = keras.Input(shape=(Nparallel,), name='parallelScalars')\n",
    "    if insteadmax:\n",
    "        # take max over the Nparallel direction to get (?,1,Nout)\n",
    "        out = layers.MaxPool1D(pool_size=Nparallel)(xb)\n",
    "        # reshape to (?,Nout)\n",
    "        out = layers.Reshape((Nout,))(out)\n",
    "    else:\n",
    "        # do a weighted sum over the Nparallel direction to get (?,Nout)\n",
    "        out = layers.Dot((-2,-1))([xb,weight_inputs])\n",
    "    \n",
    "    return keras.Model(inputs=[weight_inputs,parallel_inputs], outputs=out, name='parallelwrapper')\n",
    "if 1:\n",
    "    # manual check\n",
    "    mmd = multiDense(368,3,5,15)\n",
    "    mpw = parallelwrapper(23,mmd,insteadmax=0)\n",
    "    mpw.summary()\n",
    "    # make models\n",
    "  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531872cf-df2c-4ed1-b68e-f5982ed78c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Reshape\n",
    "from tensorflow.keras import regularizers, Model\n",
    "\n",
    "def init_generator(data, labels,\n",
    "                   baselayers, Nfeatures, endlayers,\n",
    "                   base_regularizer=None, end_regularizer=None,\n",
    "                   num_blocks=3, d_ff=64):\n",
    "    # 1) Extract dims\n",
    "    Nviews             = data[0].shape[1]   # 23\n",
    "    Natoms             = data[1].shape[2]   # 23\n",
    "    d_model            = data[1].shape[3]   # 16 \n",
    "    flattened_dim      = Natoms * d_model   # 23*16=368\n",
    "\n",
    "    # 2) Inputs\n",
    "    weight_input = Input((Nviews,), name='weight_input')             # (batch,23)\n",
    "    atom_input   = Input((Nviews, Natoms, d_model), name='atom_input')# (batch,23,23,16)\n",
    "\n",
    "    # 3) Stacked Transformer blocks\n",
    "    x = atom_input\n",
    "    for i in range(num_blocks):\n",
    "        x = TransformerBlock(\n",
    "                d_model=d_model,\n",
    "                d_k=32,\n",
    "                d_v=d_model,\n",
    "                d_ff=d_ff,\n",
    "                name=f'transformer_block_{i}'\n",
    "            )(x)\n",
    "        # x is still (batch, 23, 23, 16)\n",
    "\n",
    "    # 4) Flatten & pool\n",
    "    \n",
    "    x = Reshape((Nviews, flattened_dim), name='flatten_views')(x)  # (batch,23,368)\n",
    "    Gbase = multiDense(flattened_dim, Nfeatures, baselayers,\n",
    "                       kernel_regularizer=base_regularizer)\n",
    "    Gpw   = parallelwrapper(Nviews, Gbase, insteadmax=False)\n",
    "    x     = Gpw([weight_input, x])                                 # (batch,Nfeatures)\n",
    "\n",
    "    # 5) Final MLP \n",
    "    Gft    = multiDense(Nfeatures, 1, endlayers,\n",
    "                        kernel_regularizer=end_regularizer)\n",
    "    output = Gft(x)                                                # (batch,1)\n",
    "\n",
    "    # 6) Assemble & compile\n",
    "    generator = Model([weight_input, atom_input], output,\n",
    "                      name='generator_with_attention')\n",
    "    generator.compile(optimizer='adam', loss='mse')\n",
    "    return generator\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "baselayers = 2\n",
    "base_reg = 0\n",
    "Nfeatures = 3\n",
    "endlayers = 2\n",
    "end_reg = 0\n",
    "\n",
    "# Regularizers\n",
    "base_regularizer = regularizers.l2(base_reg) if base_reg else None\n",
    "end_regularizer = regularizers.l2(end_reg) if end_reg else None\n",
    "\n",
    "print(\"Model configuration:\")\n",
    "print(f\"baselayers={baselayers}, base_reg={base_reg}, Nfeatures={Nfeatures}, endlayers={endlayers}, end_reg={end_reg}\")\n",
    "\n",
    "# Initialize generator\n",
    "generator = init_generator(dataG_train, labelsG_train, baselayers, Nfeatures, endlayers,\n",
    "                          base_regularizer=base_regularizer,\n",
    "                          end_regularizer=end_regularizer)\n",
    "\n",
    "# Data preparation\n",
    "dataG_train = [\n",
    "    np.array(dataG_train[0], dtype='float32'),  # weights (5732, 23)\n",
    "    np.array(dataG_train[1], dtype='float32')   # atom features (5732, 23, 23, 16)\n",
    "]\n",
    "labelsG_train = np.array(labelsG_train, dtype='float32').reshape(-1, 1)\n",
    "\n",
    "# Test data\n",
    "dataG_test = [\n",
    "    np.array(dataG_test[0], dtype='float32'),  # weights (1433, 23)\n",
    "    np.array(dataG_test[1], dtype='float32')   # atom features (1433, 23, 23, 16)\n",
    "]\n",
    "labelsG_test = np.array(labelsG_test, dtype='float32').reshape(-1, 1)\n",
    "\n",
    "# Verify dimensions\n",
    "print(\"\\nFinal data shapes:\")\n",
    "print(f\"Train weights: {dataG_train[0].shape}\")\n",
    "print(f\"Train atom features: {dataG_train[1].shape}\")\n",
    "print(f\"Train labels: {labelsG_train.shape}\")\n",
    "print(f\"Test weights: {dataG_test[0].shape}\")\n",
    "print(f\"Test atom features: {dataG_test[1].shape}\")\n",
    "print(f\"Test labels: {labelsG_test.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e94a9b-ab07-4042-b3d7-a55b49dfd761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ba451b-bc3f-4ceb-ba97-22819e082f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# fit\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=7,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "if 1:\n",
    "    history = generator.fit(\n",
    "        dataG_train,\n",
    "        labelsG_train,\n",
    "        epochs=50,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        validation_data=(dataG_test, labelsG_test),\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "    print(\"train loss =\", generator.evaluate(dataG_train, labelsG_train, verbose=1))\n",
    "    print(\"test loss  =\", generator.evaluate(dataG_test,  labelsG_test,  verbose=1))\n",
    "#    generator.save(modelpath + \"AG-model-RT.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d5ea4e-f0b4-4b5f-a150-d690519bbecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_train = generator.predict(dataG_train)\n",
    "predicted_test = generator.predict(dataG_test)\n",
    "\n",
    "# Calculate the Mean Absolute Error for the training set\n",
    "mae_train = np.mean(np.abs(predicted_train - labelsG_train))\n",
    "\n",
    "# Calculate the Mean Absolute Error for the test set\n",
    "mae_test = np.mean(np.abs(predicted_test - labelsG_test))\n",
    "\n",
    "# Print out the MAE for both sets\n",
    "print(\"Train Mean Absolute Error: \", mae_train)\n",
    "print(\"Test Mean Absolute Error: \", mae_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc74cf2e-d42a-48b1-b31e-a55c3e46ecda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
